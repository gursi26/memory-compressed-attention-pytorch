{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<unk>\"\n",
    "special_tokens = [\"<unk>\", \"<sep>\", \"<eos>\", \"<pad>\"]\n",
    "tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=special_tokens, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(\n",
    "    files=[\"text.txt\"],\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save(\"tokenizer.json\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.unk_token = unk_token\n",
    "tokenizer.eos_token = \"<eos>\"\n",
    "tokenizer.sep_token = \"<sep>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16665,    15,   851,   422,   614,  1864,  1830,    34,     2,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3],\n",
       "        [  686,   355,  5910,   620,   369,   925,   358,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3],\n",
       "        [  494,   369,    42,  1051,  2621,  3541,   625,   355,   648,   621,\n",
       "           463,   355,  1950,   559,   379,    57,   863,   555]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"hello, how are you doing today? <eos>\", \"what the fuck is going on\", \"this is a much longer sentence than the last two so the others can be padded more\"], return_tensors=\"pt\", padding=True).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, how are you doing today? <eos> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([16665, 15, 851, 422, 614, 1864, 1830, 34, 2, 3, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
